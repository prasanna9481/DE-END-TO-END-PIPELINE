{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB database and table created successfully.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a new DuckDB database (or connect to an existing one)\n",
    "conn = duckdb.connect('cleaned_redshift_data_001.duckdb')\n",
    "\n",
    "# Define the schema and create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE queries (\n",
    "    instance_id VARCHAR,\n",
    "    cluster_size INTEGER,\n",
    "    user_id VARCHAR,\n",
    "    database_id VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    arrival_timestamp TIMESTAMP,\n",
    "    compile_duration_ms INTEGER,\n",
    "    queue_duration_ms INTEGER,\n",
    "    execution_duration_ms INTEGER,\n",
    "    feature_fingerprint VARCHAR,\n",
    "    was_aborted BOOLEAN,\n",
    "    was_cached BOOLEAN,\n",
    "    cache_source_query_id VARCHAR,\n",
    "    query_type VARCHAR,\n",
    "    num_permanent_tables_accessed INTEGER,\n",
    "    num_external_tables_accessed INTEGER,\n",
    "    num_system_tables_accessed INTEGER,\n",
    "    read_table_ids VARCHAR,\n",
    "    write_table_ids VARCHAR,\n",
    "    mbytes_scanned INTEGER,\n",
    "    mbytes_spilled INTEGER,\n",
    "    num_joins INTEGER,\n",
    "    num_scans INTEGER,\n",
    "    num_aggregations INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "conn.execute(create_table_query)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"DuckDB database and table created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded, cleaned, and inserted into DuckDB.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "def load_and_clean_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Define conversion functions\n",
    "    def convert_to_string(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return str(s).strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_integer(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return int(float(s))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_float(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_datetime(s):\n",
    "        datetime_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S.%f',  # Format in your data\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d-%m-%Y',\n",
    "            '%m/%d/%Y',\n",
    "            '%Y/%m/%d',\n",
    "            '%d-%b-%Y'\n",
    "        ]\n",
    "        for fmt in datetime_formats:\n",
    "            try:\n",
    "                return pd.to_datetime(s, format=fmt)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "    def convert_to_boolean(s):\n",
    "        if pd.isna(s) or s == '':\n",
    "            return None\n",
    "        if str(s).lower() in ['true', '1', 'yes']:\n",
    "            return True\n",
    "        elif str(s).lower() in ['false', '0', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Define column conversions\n",
    "    column_conversions = {\n",
    "        'instance_id': convert_to_string,\n",
    "        'cluster_size': convert_to_integer,\n",
    "        'user_id': convert_to_string,\n",
    "        'database_id': convert_to_string,\n",
    "        'query_id': convert_to_string,\n",
    "        'arrival_timestamp': convert_to_datetime,\n",
    "        'compile_duration_ms': convert_to_integer,\n",
    "        'queue_duration_ms': convert_to_integer,\n",
    "        'execution_duration_ms': convert_to_integer,\n",
    "        'feature_fingerprint': convert_to_string,\n",
    "        'was_aborted': convert_to_boolean,\n",
    "        'was_cached': convert_to_boolean,\n",
    "        'cache_source_query_id': convert_to_string,\n",
    "        'query_type': convert_to_string,\n",
    "        'num_permanent_tables_accessed': convert_to_integer,\n",
    "        'num_external_tables_accessed': convert_to_integer,\n",
    "        'num_system_tables_accessed': convert_to_integer,\n",
    "        'read_table_ids': convert_to_string,\n",
    "        'write_table_ids': convert_to_string,\n",
    "        'mbytes_scanned': convert_to_integer,\n",
    "        'mbytes_spilled': convert_to_integer,\n",
    "        'num_joins': convert_to_integer,\n",
    "        'num_scans': convert_to_integer,\n",
    "        'num_aggregations': convert_to_integer\n",
    "    }\n",
    "\n",
    "    # Apply conversions\n",
    "    for col, func in column_conversions.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(func)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Optionally, drop rows with missing essential values (e.g., 'instance_id', 'query_id')\n",
    "    # df.dropna(subset=['instance_id', 'query_id'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def insert_into_duckdb(df, duckdb_file='cleaned_redshift_data_001.duckdb'):\n",
    "    # Connect to DuckDB\n",
    "    conn = duckdb.connect(duckdb_file)\n",
    "\n",
    "    # Insert data into the existing 'queries' table\n",
    "    # If the table doesn't exist, you can create it here or ensure it's created beforehand\n",
    "    try:\n",
    "        conn.execute(\"INSERT INTO queries SELECT * FROM df\")\n",
    "    except duckdb.Error as e:\n",
    "        print(\"An error occurred while inserting data into DuckDB:\", e)\n",
    "        print(\"Attempting to create the table and retry insertion.\")\n",
    "        # Define the schema and create the table if it doesn't exist\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE queries (\n",
    "            instance_id VARCHAR,\n",
    "            cluster_size INTEGER,\n",
    "            user_id VARCHAR,\n",
    "            database_id VARCHAR,\n",
    "            query_id VARCHAR,\n",
    "            arrival_timestamp TIMESTAMP,\n",
    "            compile_duration_ms INTEGER,\n",
    "            queue_duration_ms INTEGER,\n",
    "            execution_duration_ms INTEGER,\n",
    "            feature_fingerprint VARCHAR,\n",
    "            was_aborted BOOLEAN,\n",
    "            was_cached BOOLEAN,\n",
    "            cache_source_query_id VARCHAR,\n",
    "            query_type VARCHAR,\n",
    "            num_permanent_tables_accessed INTEGER,\n",
    "            num_external_tables_accessed INTEGER,\n",
    "            num_system_tables_accessed INTEGER,\n",
    "            read_table_ids VARCHAR,\n",
    "            write_table_ids VARCHAR,\n",
    "            mbytes_scanned INTEGER,\n",
    "            mbytes_spilled INTEGER,\n",
    "            num_joins INTEGER,\n",
    "            num_scans INTEGER,\n",
    "            num_aggregations INTEGER\n",
    "        );\n",
    "        \"\"\"\n",
    "        conn.execute(create_table_query)\n",
    "        conn.execute(\"INSERT INTO queries SELECT * FROM df\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    csv_file = 'sample_0.01.csv'\n",
    "    df = load_and_clean_data(csv_file)\n",
    "    insert_into_duckdb(df)\n",
    "    print(\"Data successfully loaded, cleaned, and inserted into DuckDB.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer is shutting down...\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "def run_consumer():\n",
    "    # Kafka Configuration\n",
    "    kafka_config = {\n",
    "        'bootstrap.servers': 'dep-eng-data-s-heimgarten.hosts.utn.de:9092',\n",
    "        'group.id': f'hello_group_{random.randint(1, 1000000)}',\n",
    "        'auto.offset.reset': 'latest'\n",
    "    }\n",
    "    \n",
    "    # Create Consumer\n",
    "    consumer = Consumer(kafka_config)\n",
    "    \n",
    "    # Subscribe to Topic\n",
    "    topic = 'chache-me-if-you-can'\n",
    "    consumer.subscribe([topic])\n",
    "    \n",
    "    # Connect to DuckDB\n",
    "    conn = duckdb.connect('my_database.duckdb')\n",
    "\n",
    "    # Create a list to store valid rows\n",
    "    valid_rows = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for messages (timeout 1 second)\n",
    "            msg = consumer.poll(1.0)\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "                \n",
    "            if msg.error():\n",
    "                print(f'Consumer error: {msg.error()}')\n",
    "                continue\n",
    "                \n",
    "            # Decode the message value\n",
    "            message = msg.value().decode(\"utf-8\")\n",
    "            print(f\"Received message: {message}\")\n",
    "            row = json.loads(message)  # Convert JSON string to dictionary\n",
    "            \n",
    "            # Check for missing values\n",
    "            if all(value is not None for value in row.values()):\n",
    "                valid_rows.append(row)  # Append valid rows to the list\n",
    "            \n",
    "            # If we have a batch of valid rows, save them to DuckDB\n",
    "            if len(valid_rows) >= 10:  # Adjust batch size as needed\n",
    "                df = pd.DataFrame(valid_rows)\n",
    "                df.to_sql('queries', conn, if_exists='append', index=False)\n",
    "                valid_rows = []  # Reset the list after saving\n",
    "                print(\"Batch of rows saved to DuckDB.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer is shutting down...\")\n",
    "    finally:\n",
    "        # Save any remaining valid rows\n",
    "        if valid_rows:\n",
    "            df = pd.DataFrame(valid_rows)\n",
    "            df.to_sql('queries', conn, if_exists='append', index=False)\n",
    "        consumer.close()\n",
    "        conn.close()\n",
    "        print(\"Consumer and DuckDB connection closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_consumer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  instance_id  cluster_size user_id database_id query_id  \\\n",
      "0         104           NaN       0           0   143296   \n",
      "1          19           NaN       0           0   299859   \n",
      "2          85           NaN       0           0    66546   \n",
      "3          80           NaN       0           0    35238   \n",
      "4         134           NaN       0           0   489975   \n",
      "\n",
      "           arrival_timestamp  compile_duration_ms  queue_duration_ms  \\\n",
      "0 2024-03-01 00:02:17.633593               760245                  0   \n",
      "1 2024-03-01 00:02:19.865071                   57                  0   \n",
      "2 2024-03-01 00:03:35.194632                  418                  0   \n",
      "3 2024-03-01 00:07:56.531040                 5343                  0   \n",
      "4 2024-03-01 00:08:15.041434                 1091                  0   \n",
      "\n",
      "   execution_duration_ms                                feature_fingerprint  \\\n",
      "0                 780366  f93d7a158afa58c0981874d43045b0978c5da8a0d7accf...   \n",
      "1                     61  cd6f43ddc6e52d0acb4f216c9f7718f215e9bd6c4f22f2...   \n",
      "2                    436  fa4d90562831d29483793f08512f53047a7ad5cf508db1...   \n",
      "3                   5439  3b4a778aead943e7e8849b50f180a4bf5d23f4a6d93e07...   \n",
      "4                   1094  c019dceb3ae0ca717667f3f9c14d518cbd001482decff0...   \n",
      "\n",
      "   ...  num_permanent_tables_accessed  num_external_tables_accessed  \\\n",
      "0  ...                              2                             0   \n",
      "1  ...                              3                             0   \n",
      "2  ...                              0                             0   \n",
      "3  ...                              0                             0   \n",
      "4  ...                              2                             0   \n",
      "\n",
      "  num_system_tables_accessed read_table_ids  write_table_ids  mbytes_scanned  \\\n",
      "0                          0              6             None          124716   \n",
      "1                          0         101124          79093.0            1159   \n",
      "2                         26           None             None               0   \n",
      "3                          5           None             None               0   \n",
      "4                          0     177114,462            462.0             632   \n",
      "\n",
      "   mbytes_spilled num_joins num_scans  num_aggregations  \n",
      "0               0         1         2                 3  \n",
      "1               0         0         1                 2  \n",
      "2               0        14         0                20  \n",
      "3               0         1         0                 3  \n",
      "4               0         1         2                 2  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to the DuckDB database\n",
    "conn = duckdb.connect('cleaned_redshift_data_001.duckdb')\n",
    "\n",
    "# Define a simple SQL query\n",
    "query = \"SELECT * FROM queries LIMIT 5;\"\n",
    "\n",
    "# Execute the query and fetch results\n",
    "result = conn.execute(query).fetchdf()  # Fetch result as a DataFrame\n",
    "\n",
    "# Print the results\n",
    "print(result)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
