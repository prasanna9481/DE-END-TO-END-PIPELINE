{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB database and table created successfully.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a new DuckDB database (or connect to an existing one)\n",
    "conn = duckdb.connect('redshift_queries.duckdb')\n",
    "\n",
    "# Define the schema and create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE live_queries (\n",
    "    instance_id VARCHAR,\n",
    "    cluster_size INTEGER,\n",
    "    user_id VARCHAR,\n",
    "    database_id VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    arrival_timestamp TIMESTAMP,\n",
    "    compile_duration_ms INTEGER,\n",
    "    queue_duration_ms INTEGER,\n",
    "    execution_duration_ms INTEGER,\n",
    "    feature_fingerprint VARCHAR,\n",
    "    was_aborted BOOLEAN,\n",
    "    was_cached BOOLEAN,\n",
    "    cache_source_query_id VARCHAR,\n",
    "    query_type VARCHAR,\n",
    "    num_permanent_tables_accessed INTEGER,\n",
    "    num_external_tables_accessed INTEGER,\n",
    "    num_system_tables_accessed INTEGER,\n",
    "    read_table_ids VARCHAR,\n",
    "    write_table_ids VARCHAR,\n",
    "    mbytes_scanned INTEGER,\n",
    "    mbytes_spilled INTEGER,\n",
    "    num_joins INTEGER,\n",
    "    num_scans INTEGER,\n",
    "    num_aggregations INTEGER,\n",
    "    dataset_type VARCHAR,\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "conn.execute(\"DROP TABLE IF EXISTS live_queries\")\n",
    "conn.execute(create_table_query)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"DuckDB database and table created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('redshift_queries.duckdb')\n",
    "#add parquet file to duckdb combined_sorted_redset_datasets.parquet that contains the data\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO live_queries\n",
    "SELECT * FROM parquet_scan('combined_sorted_redset_datasets.parquet')\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to insert the data  \n",
    "conn.execute(insert_query)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "def load_and_clean_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Define conversion functions\n",
    "    def convert_to_string(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return str(s).strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_integer(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return int(float(s))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_float(s):\n",
    "        try:\n",
    "            if pd.isna(s) or s == '':\n",
    "                return None\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def convert_to_datetime(s):\n",
    "        datetime_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S.%f',  # Format in your data\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d-%m-%Y',\n",
    "            '%m/%d/%Y',\n",
    "            '%Y/%m/%d',\n",
    "            '%d-%b-%Y'\n",
    "        ]\n",
    "        for fmt in datetime_formats:\n",
    "            try:\n",
    "                return pd.to_datetime(s, format=fmt)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "    def convert_to_boolean(s):\n",
    "        if pd.isna(s) or s == '':\n",
    "            return None\n",
    "        if str(s).lower() in ['true', '1', 'yes']:\n",
    "            return True\n",
    "        elif str(s).lower() in ['false', '0', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Define column conversions\n",
    "    column_conversions = {\n",
    "        'instance_id': convert_to_string,\n",
    "        'cluster_size': convert_to_integer,\n",
    "        'user_id': convert_to_string,\n",
    "        'database_id': convert_to_string,\n",
    "        'query_id': convert_to_string,\n",
    "        'arrival_timestamp': convert_to_datetime,\n",
    "        'compile_duration_ms': convert_to_integer,\n",
    "        'queue_duration_ms': convert_to_integer,\n",
    "        'execution_duration_ms': convert_to_integer,\n",
    "        'feature_fingerprint': convert_to_string,\n",
    "        'was_aborted': convert_to_boolean,\n",
    "        'was_cached': convert_to_boolean,\n",
    "        'cache_source_query_id': convert_to_string,\n",
    "        'query_type': convert_to_string,\n",
    "        'num_permanent_tables_accessed': convert_to_integer,\n",
    "        'num_external_tables_accessed': convert_to_integer,\n",
    "        'num_system_tables_accessed': convert_to_integer,\n",
    "        'read_table_ids': convert_to_string,\n",
    "        'write_table_ids': convert_to_string,\n",
    "        'mbytes_scanned': convert_to_integer,\n",
    "        'mbytes_spilled': convert_to_integer,\n",
    "        'num_joins': convert_to_integer,\n",
    "        'num_scans': convert_to_integer,\n",
    "        'num_aggregations': convert_to_integer,\n",
    "        'dataset_type': convert_to_string,\n",
    "    }\n",
    "\n",
    "    # Apply conversions\n",
    "    for col, func in column_conversions.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(func)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Optionally, drop rows with missing essential values (e.g., 'instance_id', 'query_id')\n",
    "    # df.dropna(subset=['instance_id', 'query_id'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def insert_into_duckdb(df, duckdb_file='cleaned_redshift_data_001.duckdb'):\n",
    "    # Connect to DuckDB\n",
    "    conn = duckdb.connect(duckdb_file)\n",
    "\n",
    "    # Insert data into the existing 'queries' table\n",
    "    # If the table doesn't exist, you can create it here or ensure it's created beforehand\n",
    "    try:\n",
    "        conn.execute(\"INSERT INTO queries SELECT * FROM df\")\n",
    "    except duckdb.Error as e:\n",
    "        print(\"An error occurred while inserting data into DuckDB:\", e)\n",
    "        print(\"Attempting to create the table and retry insertion.\")\n",
    "        # Define the schema and create the table if it doesn't exist\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE queries (\n",
    "            instance_id VARCHAR,\n",
    "            cluster_size INTEGER,\n",
    "            user_id VARCHAR,\n",
    "            database_id VARCHAR,\n",
    "            query_id VARCHAR,\n",
    "            arrival_timestamp TIMESTAMP,\n",
    "            compile_duration_ms INTEGER,\n",
    "            queue_duration_ms INTEGER,\n",
    "            execution_duration_ms INTEGER,\n",
    "            feature_fingerprint VARCHAR,\n",
    "            was_aborted BOOLEAN,\n",
    "            was_cached BOOLEAN,\n",
    "            cache_source_query_id VARCHAR,\n",
    "            query_type VARCHAR,\n",
    "            num_permanent_tables_accessed INTEGER,\n",
    "            num_external_tables_accessed INTEGER,\n",
    "            num_system_tables_accessed INTEGER,\n",
    "            read_table_ids VARCHAR,\n",
    "            write_table_ids VARCHAR,\n",
    "            mbytes_scanned INTEGER,\n",
    "            mbytes_spilled INTEGER,\n",
    "            num_joins INTEGER,\n",
    "            num_scans INTEGER,\n",
    "            num_aggregations INTEGER\n",
    "        );\n",
    "        \"\"\"\n",
    "        conn.execute(create_table_query)\n",
    "        conn.execute(\"INSERT INTO queries SELECT * FROM df\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "def main():\n",
    "    csv_file = 'sample_0.01.csv'\n",
    "    df = load_and_clean_data(csv_file)\n",
    "    insert_into_duckdb(df)\n",
    "    print(\"Data successfully loaded, cleaned, and inserted into DuckDB.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "def run_consumer():\n",
    "    # Kafka Configuration\n",
    "    kafka_config = {\n",
    "        'bootstrap.servers': 'dep-eng-data-s-heimgarten.hosts.utn.de:9092',\n",
    "        'group.id': f'hello_group_{random.randint(1, 1000000)}',\n",
    "        'auto.offset.reset': 'latest'\n",
    "    }\n",
    "    \n",
    "    # Create Consumer\n",
    "    consumer = Consumer(kafka_config)\n",
    "    \n",
    "    # Subscribe to Topic\n",
    "    topic = 'chache-me-if-you-can'\n",
    "    consumer.subscribe([topic])\n",
    "    \n",
    "    # Connect to DuckDB\n",
    "    conn = duckdb.connect('my_database.duckdb')\n",
    "\n",
    "    # Create a list to store valid rows\n",
    "    valid_rows = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for messages (timeout 1 second)\n",
    "            msg = consumer.poll(1.0)\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "                \n",
    "            if msg.error():\n",
    "                print(f'Consumer error: {msg.error()}')\n",
    "                continue\n",
    "                \n",
    "            # Decode the message value\n",
    "            message = msg.value().decode(\"utf-8\")\n",
    "            print(f\"Received message: {message}\")\n",
    "            row = json.loads(message)  # Convert JSON string to dictionary\n",
    "            \n",
    "            # Check for missing values\n",
    "            if all(value is not None for value in row.values()):\n",
    "                valid_rows.append(row)  # Append valid rows to the list\n",
    "            \n",
    "            # If we have a batch of valid rows, save them to DuckDB\n",
    "            if len(valid_rows) >= 10:  # Adjust batch size as needed\n",
    "                df = pd.DataFrame(valid_rows)\n",
    "                df.to_sql('queries', conn, if_exists='append', index=False)\n",
    "                valid_rows = []  # Reset the list after saving\n",
    "                print(\"Batch of rows saved to DuckDB.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Consumer is shutting down...\")\n",
    "    finally:\n",
    "        # Save any remaining valid rows\n",
    "        if valid_rows:\n",
    "            df = pd.DataFrame(valid_rows)\n",
    "            df.to_sql('queries', conn, if_exists='append', index=False)\n",
    "        consumer.close()\n",
    "        conn.close()\n",
    "        print(\"Consumer and DuckDB connection closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_consumer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
