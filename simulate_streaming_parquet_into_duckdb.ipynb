{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a new DuckDB database (or connect to an existing one)\n",
    "conn = duckdb.connect('redshift_queries.duckdb')\n",
    "\n",
    "conn.execute(\"DELETE FROM live_queries\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new table and delete old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB database and table created successfully.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a new DuckDB database (or connect to an existing one)\n",
    "conn = duckdb.connect('redshift_queries.duckdb')\n",
    "\n",
    "# Define the schema and create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE live_queries (\n",
    "    instance_id VARCHAR,\n",
    "    cluster_size INTEGER,\n",
    "    user_id VARCHAR,\n",
    "    database_id VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    arrival_timestamp TIMESTAMP,\n",
    "    compile_duration_ms INTEGER,\n",
    "    queue_duration_ms INTEGER,\n",
    "    execution_duration_ms INTEGER,\n",
    "    feature_fingerprint VARCHAR,\n",
    "    was_aborted BOOLEAN,\n",
    "    was_cached BOOLEAN,\n",
    "    cache_source_query_id VARCHAR,\n",
    "    query_type VARCHAR,\n",
    "    num_permanent_tables_accessed INTEGER,\n",
    "    num_external_tables_accessed INTEGER,\n",
    "    num_system_tables_accessed INTEGER,\n",
    "    read_table_ids VARCHAR,\n",
    "    write_table_ids VARCHAR,\n",
    "    mbytes_scanned INTEGER,\n",
    "    mbytes_spilled INTEGER,\n",
    "    num_joins INTEGER,\n",
    "    num_scans INTEGER,\n",
    "    num_aggregations INTEGER,\n",
    "    dataset_type VARCHAR,\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "conn.execute(\"DROP TABLE IF EXISTS live_queries\")\n",
    "\n",
    "conn.execute(create_table_query)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"DuckDB database and table created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data into the table in given interval to simulate streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 12575460\n",
      "Batch size: 80\n",
      "12575460 rows loaded from combined_sorted_redset_datasets.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(INTERVAL)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Load and clean data once at startup\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_sorted_redset_datasets.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 59\u001b[0m     cleaned_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_clean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Create batch generator\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     batch_iter \u001b[38;5;241m=\u001b[39m batch_generator(cleaned_df, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mload_and_clean_data\u001b[0;34m(parquet_file)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Apply conversions\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, func \u001b[38;5;129;01min\u001b[39;00m column_conversions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns: df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Desktop/Repos/DE-Project-Redset/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repos/DE-Project-Redset/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repos/DE-Project-Redset/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/Repos/DE-Project-Redset/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repos/DE-Project-Redset/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1741\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;66;03m# we must convert to python types\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# in which speed should the 3 months be played back\n",
    "_playbackspeed = 100\n",
    "\n",
    "# 3 months of data\n",
    "_seconds = 60 * 60 * 24 * 90\n",
    "_entries = len(pd.read_parquet('combined_sorted_redset_datasets.parquet'))\n",
    "print(f\"Entries: {_entries}\")\n",
    "\n",
    "# Calculate the interval and batch size based on the playback speed\n",
    "INTERVAL = 0.5\n",
    "print(f\"Interval: {INTERVAL}\")\n",
    "\n",
    "BATCH_SIZE = int(_entries / (_seconds / INTERVAL) * _playbackspeed)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "def load_and_clean_data(parquet_file):\n",
    "    # Read the entire Parquet file once\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(len(df), \"rows loaded from\", parquet_file)\n",
    "    \n",
    "    # Define conversion functions (from your existing code)\n",
    "    def convert_to_string(s):\n",
    "        try: return str(s).strip() if not pd.isna(s) and s != '' else None\n",
    "        except: return None\n",
    "    \n",
    "    def convert_to_integer(s):\n",
    "        try: return int(float(s)) if not pd.isna(s) and s != '' else None\n",
    "        except: return None\n",
    "    \n",
    "    # ... include all other conversion functions from your code ...\n",
    "\n",
    "    # Define column conversions\n",
    "    column_conversions = {\n",
    "        'instance_id': convert_to_string,\n",
    "        'cluster_size': convert_to_integer,\n",
    "        # ... include all other columns ...\n",
    "    }\n",
    "\n",
    "    # Apply conversions\n",
    "    for col, func in column_conversions.items():\n",
    "        if col in df.columns: df[col] = df[col].apply(func)\n",
    "        else: print(f\"Warning: Column '{col}' not found\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def batch_generator(df, batch_size=10):\n",
    "    \"\"\"Yield batches of cleaned data from the DataFrame\"\"\"\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size]\n",
    "\n",
    "def main():\n",
    "    # Load and clean data once at startup\n",
    "    parquet_file = 'combined_sorted_redset_datasets.parquet'\n",
    "    cleaned_df = load_and_clean_data(parquet_file)\n",
    "    \n",
    "    # Create batch generator\n",
    "    batch_iter = batch_generator(cleaned_df, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Continuously insert batches\n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batch_iter)\n",
    "            conn = duckdb.connect('redshift_queries.duckdb')\n",
    "            \n",
    "            # Register the batch as a temporary DuckDB table\n",
    "            conn.register('current_batch', batch)\n",
    "            \n",
    "            # Insert into live_queries\n",
    "            conn.execute(\"\"\"\n",
    "                INSERT INTO live_queries \n",
    "                SELECT * FROM current_batch\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "            print(f\"{time.ctime()}: Inserted {len(batch)} rows into live_queries\")\n",
    "            \n",
    "        except StopIteration:\n",
    "            print(\"All data processed. Exiting...\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Writer Error: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            if 'conn' in locals(): conn.close()\n",
    "        \n",
    "        time.sleep(INTERVAL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
