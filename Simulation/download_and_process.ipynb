{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data, sort after date , split into static and streaming data, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serverless data already exists.\n",
      "Provisioned data already exists.\n",
      "Loading serverless dataset...\n",
      "Loading provisioned dataset...\n",
      "Combining datasets...\n",
      "Sorting by arrival_timestamp...\n",
      "Dropping duplicates...\n",
      "Static dataset - Start day: 2024-03-01 00:00:05.086395, End day: 2024-04-30 11:18:57.975306\n",
      "Saving static_data.parquet...\n",
      "Streaming dataset - Start day: 2024-04-30 11:18:59.069925, End day: 2024-05-30 23:59:42.680457\n",
      "Saving streaming_data.parquet...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc  # Import garbage collector interface\n",
    "\n",
    "\n",
    "# Check if the data is already downloaded\n",
    "if not os.path.exists(\"sample_0.01_serverless.parquet\"):\n",
    "      !aws s3 cp --no-sign-request s3://redshift-downloads/redset/serverless/sample_0.01.parquet sample_0.01_serverless.parquet\n",
    "      print(\"Serverless data downloaded.\")\n",
    "else:\n",
    "      print(\"Serverless data already exists.\")\n",
    "\n",
    "if not os.path.exists(\"sample_0.01_provisioned.parquet\"):\n",
    "      !aws s3 cp --no-sign-request s3://redshift-downloads/redset/provisioned/sample_0.01.parquet sample_0.01_provisioned.parquet\n",
    "      print(\"Provisioned data downloaded.\")\n",
    "else:\n",
    "      print(\"Provisioned data already exists.\")\n",
    "\n",
    "\n",
    "# Load the serverless dataset\n",
    "print(\"Loading serverless dataset...\")\n",
    "serverless_df = pd.read_parquet(\"sample_0.01_serverless.parquet\")\n",
    "serverless_df['dataset_type'] = 'serverless'  # Add flag for serverless\n",
    "\n",
    "# Load the provisioned dataset\n",
    "print(\"Loading provisioned dataset...\")\n",
    "provisioned_df = pd.read_parquet(\"sample_0.01_provisioned.parquet\")\n",
    "provisioned_df['dataset_type'] = 'provisioned'  # Add flag for provisioned\n",
    "\n",
    "# Combine both datasets into one DataFrame\n",
    "print(\"Combining datasets...\")\n",
    "combined_df = pd.concat([serverless_df, provisioned_df], ignore_index=True)\n",
    "\n",
    "# Clear memory of the individual datasets as they're no longer needed\n",
    "del serverless_df\n",
    "del provisioned_df\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "# Sort the combined dataset by 'arrival_timestamp'\n",
    "print(\"Sorting by arrival_timestamp...\")\n",
    "combined_df = combined_df.sort_values(by='arrival_timestamp')\n",
    "\n",
    "# Drop duplicate records\n",
    "print(\"Dropping duplicates...\")\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Calculate the index for 66% of the DataFrame\n",
    "split_index = int(len(combined_df) * 0.66)\n",
    "\n",
    "# Split the DataFrame into static_data and streaming_data\n",
    "static_data = combined_df.iloc[:split_index]\n",
    "\n",
    "# Display information about the static dataset\n",
    "print(f\"Static dataset - Start day: {static_data['arrival_timestamp'].iloc[0]}, \"\n",
    "      f\"End day: {static_data['arrival_timestamp'].iloc[-1]}\")\n",
    "\n",
    "# Save the static data to a Parquet file\n",
    "print(\"Saving static_data.parquet...\")\n",
    "static_data.to_parquet(\"static_data.parquet\", index=False)\n",
    "\n",
    "# Clear memory of static_data\n",
    "del static_data\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "# Take the remaining 34% for the streaming dataset\n",
    "streaming_data = combined_df.iloc[split_index:]\n",
    "\n",
    "# Display information about the streaming dataset\n",
    "print(f\"Streaming dataset - Start day: {streaming_data['arrival_timestamp'].iloc[0]}, \"\n",
    "      f\"End day: {streaming_data['arrival_timestamp'].iloc[-1]}\")\n",
    "\n",
    "# Save the streaming data to a Parquet file\n",
    "print(\"Saving streaming_data.parquet...\")\n",
    "streaming_data.to_parquet(\"streaming_data.parquet\", index=False)\n",
    "\n",
    "# Clear memory of streaming_data\n",
    "del streaming_data\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "# Clear the combined DataFrame from memory\n",
    "del combined_df\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datatypes in Static Data to upload directly to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 292/292 [00:17<00:00, 17.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data written to static_data_ready2upload.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to the input and output Parquet files\n",
    "INPUT_PARQUET_FILE = 'static_data.parquet'  # Replace with your input file path\n",
    "OUTPUT_PARQUET_FILE = 'static_data_ready2upload.parquet'  # Replace with your desired output file path\n",
    "\n",
    "# Function to process a batch\n",
    "def process_batch(batch):\n",
    "    \"\"\"Clean and convert a single batch\"\"\"\n",
    "    # Define expected types for each column (modify according to your schema)\n",
    "    column_types = {\n",
    "        'instance_id': 'str',\n",
    "        'cluster_size': 'int',\n",
    "        'user_id': 'str',\n",
    "        'database_id': 'str',\n",
    "        'query_id': 'str',\n",
    "        'arrival_timestamp': 'datetime',\n",
    "        'compile_duration_ms': 'int',\n",
    "        'queue_duration_ms': 'int',\n",
    "        'execution_duration_ms': 'int',\n",
    "        'feature_fingerprint': 'str',\n",
    "        'was_aborted': 'bool',\n",
    "        'was_cached': 'bool',\n",
    "        'cache_source_query_id': 'str',\n",
    "        'query_type': 'str',\n",
    "        'num_permanent_tables_accessed': 'int',\n",
    "        'num_external_tables_accessed': 'int',\n",
    "        'num_system_tables_accessed': 'int',\n",
    "        'read_table_ids': 'str',\n",
    "        'write_table_ids': 'str',\n",
    "        'mbytes_scanned': 'int',\n",
    "        'mbytes_spilled': 'int',\n",
    "        'num_joins': 'int',\n",
    "        'num_scans': 'int',\n",
    "        'num_aggregations': 'int',\n",
    "        'dataset_type': 'str'\n",
    "    }\n",
    "\n",
    "    for col in batch.columns:\n",
    "        col_type = column_types.get(col, 'str')\n",
    "        \n",
    "        # Handle null values\n",
    "        if col_type in ['int', 'float']:\n",
    "            batch[col] = batch[col].fillna(0).astype(col_type)\n",
    "        elif col_type == 'bool':\n",
    "            batch[col] = batch[col].fillna(False).astype(bool)\n",
    "        elif col_type == 'datetime':\n",
    "            batch[col] = pd.to_datetime(batch[col], errors='coerce')\n",
    "            batch[col] = batch[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:  # string\n",
    "            batch[col] = batch[col].astype(str).str.strip()\n",
    "            batch[col] = batch[col].replace({'nan': '', 'None': '', 'null': ''})\n",
    "            batch[col] = batch[col].fillna('')\n",
    "    return batch\n",
    "\n",
    "# Function to process the entire Parquet file in batches\n",
    "def process_parquet_file(input_file, output_file, batch_size=10000):\n",
    "    # Open the Parquet file using ParquetFile\n",
    "    parquet_file = pq.ParquetFile(input_file)\n",
    "    \n",
    "    # Get total number of rows\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size  # Total number of batches\n",
    "    \n",
    "    # Prepare to write to the output Parquet file\n",
    "    writer = None\n",
    "    \n",
    "    # Progress bar\n",
    "    with tqdm(total=num_batches, desc='Processing batches', unit='batch') as pbar:\n",
    "        # Read and process each batch\n",
    "        for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "            # Convert the batch (RecordBatch) to a pandas DataFrame\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            # Process the batch\n",
    "            processed_batch = process_batch(df_batch)\n",
    "            \n",
    "            # Convert the processed batch back to a RecordBatch\n",
    "            table = pa.Table.from_pandas(processed_batch)\n",
    "            \n",
    "            # Initialize the Parquet writer with schema on first batch\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_file, table.schema)\n",
    "            \n",
    "            # Write the batch to the output Parquet file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Close the Parquet writer\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"Processed data written to {output_file}\")\n",
    "\n",
    "# Call the function to process the file\n",
    "process_parquet_file(INPUT_PARQUET_FILE, OUTPUT_PARQUET_FILE, batch_size=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
