{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "# to use create a .env file in the root directory and insert the following:\n",
    "# SNOWFLAKE_PASSWORD=your_password\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "\n",
    "# Playback configuration\n",
    "PLAYBACKSPEED = 10\n",
    "FILEPATH = '../combined_sorted_redset_datasets.parquet'\n",
    "INTERVAL = 5  # seconds\n",
    "\n",
    "\n",
    "# Snowflake connection configuration\n",
    "config = {\n",
    "    \"account\": \"SFEDU02-KFB85562\",\n",
    "    \"user\": \"BISON\",\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"role\": \"TRAINING_ROLE\",\n",
    "    \"warehouse\": \"ANIMAL_TASK_WH\",\n",
    "    \"database\": \"CATCH_ME_IF_YOU_CAN\",\n",
    "    \"schema\": \"PUBLIC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new table and delete old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(**config)\n",
    "\n",
    "# Define the schema and create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE live_queries (\n",
    "    instance_id VARCHAR,\n",
    "    cluster_size INTEGER,\n",
    "    user_id VARCHAR,\n",
    "    database_id VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    arrival_timestamp TIMESTAMP,\n",
    "    compile_duration_ms INTEGER,\n",
    "    queue_duration_ms INTEGER,\n",
    "    execution_duration_ms INTEGER,\n",
    "    feature_fingerprint VARCHAR,\n",
    "    was_aborted BOOLEAN,\n",
    "    was_cached BOOLEAN,\n",
    "    cache_source_query_id VARCHAR,\n",
    "    query_type VARCHAR,\n",
    "    num_permanent_tables_accessed INTEGER,\n",
    "    num_external_tables_accessed INTEGER,\n",
    "    num_system_tables_accessed INTEGER,\n",
    "    read_table_ids VARCHAR,\n",
    "    write_table_ids VARCHAR,\n",
    "    mbytes_scanned INTEGER,\n",
    "    mbytes_spilled INTEGER,\n",
    "    num_joins INTEGER,\n",
    "    num_scans INTEGER,\n",
    "    num_aggregations INTEGER,\n",
    "    dataset_type VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "cur = conn.cursor()\n",
    "cur.execute(create_table_query)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Snowflake table created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data into the table in given interval to simulate streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seconds = 60 * 60 * 24 * 90  # 3 months in seconds\n",
    "\n",
    "# Get total entries without loading full file\n",
    "_parquet_file = pq.ParquetFile(FILEPATH)\n",
    "_entries = _parquet_file.metadata.num_rows\n",
    "print(f\"Entries: {_entries}\")\n",
    "\n",
    "# Calculate batch parameters\n",
    "\n",
    "_batchsize = max(1, int(_entries / (_seconds / INTERVAL) * PLAYBACKSPEED))\n",
    "print(f\"Interval: {INTERVAL}\\nBatch size: {_batchsize}\")\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"Clean and convert a single batch\"\"\"\n",
    "    # Define expected types for each column (modify according to your schema)\n",
    "    column_types = {\n",
    "        'instance_id': 'str',\n",
    "        'cluster_size': 'int',\n",
    "        'user_id': 'str',\n",
    "        'database_id': 'str',\n",
    "        'query_id': 'str',\n",
    "        'arrival_timestamp': 'datetime',\n",
    "        'compile_duration_ms': 'int',\n",
    "        'queue_duration_ms': 'int',\n",
    "        'execution_duration_ms': 'int',\n",
    "        'feature_fingerprint': 'str',\n",
    "        'was_aborted': 'bool',\n",
    "        'was_cached': 'bool',\n",
    "        'cache_source_query_id': 'str',\n",
    "        'query_type': 'str',\n",
    "        'num_permanent_tables_accessed': 'int',\n",
    "        'num_external_tables_accessed': 'int',\n",
    "        'num_system_tables_accessed': 'int',\n",
    "        'read_table_ids': 'str',\n",
    "        'write_table_ids': 'str',\n",
    "        'mbytes_scanned': 'int',\n",
    "        'mbytes_spilled': 'int',\n",
    "        'num_joins': 'int',\n",
    "        'num_scans': 'int',\n",
    "        'num_aggregations': 'int',\n",
    "        'dataset_type': 'str'\n",
    "    }\n",
    "\n",
    "    for col in batch.columns:\n",
    "        col_type = column_types.get(col, 'str')\n",
    "        \n",
    "        # Handle null values\n",
    "        if col_type in ['int', 'float']:\n",
    "            batch[col] = batch[col].fillna(0).astype(col_type)\n",
    "        elif col_type == 'bool':\n",
    "            batch[col] = batch[col].fillna(False).astype(bool)\n",
    "        elif col_type == 'datetime':\n",
    "            batch[col] = pd.to_datetime(batch[col], errors='coerce')\n",
    "            batch[col] = batch[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:  # string\n",
    "            batch[col] = batch[col].astype(str).str.strip()\n",
    "            batch[col] = batch[col].replace({'nan': '', 'None': '', 'null': ''})\n",
    "            batch[col] = batch[col].fillna('')\n",
    "    return batch\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing loop\"\"\"\n",
    "    parquet_file = pq.ParquetFile(FILEPATH)\n",
    "    total_batches = _entries // _batchsize + 1\n",
    "    \n",
    "    with snowflake.connector.connect(**config) as conn, conn.cursor() as cur:\n",
    "        while True:\n",
    "            # Create a tqdm progress bar for our batch iteration\n",
    "            pbar = tqdm(\n",
    "                parquet_file.iter_batches(batch_size=_batchsize), \n",
    "                total=total_batches, \n",
    "                desc=\"Processing Batches\", \n",
    "                unit=\"batch\"\n",
    "            )\n",
    "            \n",
    "            for i, arrow_batch in enumerate(pbar, start=1):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Convert Arrow batch to pandas DataFrame\n",
    "                    raw_batch = arrow_batch.to_pandas()\n",
    "                    start_processing = time.time()\n",
    "                    batch = process_batch(raw_batch)\n",
    "                    process_time = time.time() - start_processing\n",
    "\n",
    "                    rows = [tuple(row) for row in batch.to_numpy()]\n",
    "\n",
    "                    # Generate bulk INSERT query\n",
    "                    columns = ', '.join(batch.columns)\n",
    "                    placeholders_per_row = ', '.join(['%s'] * len(batch.columns))\n",
    "                    values_placeholders = ', '.join([f'({placeholders_per_row})' for _ in rows])\n",
    "                    sql = f\"INSERT INTO live_queries ({columns}) VALUES {values_placeholders}\"\n",
    "\n",
    "                    # Flatten rows into a single parameter list\n",
    "                    params = [param for row in rows for param in row]\n",
    "\n",
    "                    # Execute in one go\n",
    "                    start_execute = time.time()\n",
    "                    cur.execute(sql, params)\n",
    "                    conn.commit()\n",
    "                    execute_time = time.time() - start_execute\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # If there's an error, display it once:\n",
    "                    pbar.write(f\"Error processing batch {i}: {str(e)}\")\n",
    "                \n",
    "                # Sleep if needed\n",
    "                elapsed = time.time() - start_time\n",
    "                sleep_time = max(0, INTERVAL - elapsed)\n",
    "                time.sleep(sleep_time)\n",
    "                \n",
    "                # Update progress bar postfix with some info\n",
    "                pbar.set_postfix_str(\n",
    "                    f\"Sleeptime: {sleep_time:.2f}s, process_time: {process_time:.2f}s, execute_time: {execute_time:.2f}s\"\n",
    "                )\n",
    "\n",
    "            pbar.close()\n",
    "            \n",
    "            # Once a full pass of the file is done, delete the table data and start again\n",
    "            cur.execute(\"DELETE FROM live_queries\")\n",
    "            conn.commit()\n",
    "            print(\"Table data deleted, starting again\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
