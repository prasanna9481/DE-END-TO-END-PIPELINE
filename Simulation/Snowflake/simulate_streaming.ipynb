{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake Realtime Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Entries: 1500598\n",
      "Simulation Duration: 60 minutes\n",
      "Interval Between Inserts: 5 seconds\n",
      "Original Data Time Span: 2637643.610532 seconds\n",
      "Compression Factor: 732.6787807033334\n",
      "Simulation Data Starts and Static Data Ends at: 2024-04-30 11:18:59\n",
      "Deleted 0 rows from previous simulation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbba65f5a715426eb573e3212f3f9c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Simulation Time:   0%|          | 0/721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/npjt_c8j7576_j3r6s8d71fc0000gp/T/ipykernel_53020/449568635.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_interval_data['arrival_timestamp'] = current_interval_data['arrival_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
      "/var/folders/41/npjt_c8j7576_j3r6s8d71fc0000gp/T/ipykernel_53020/449568635.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_interval_data['arrival_timestamp'] = current_interval_data['arrival_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
      "/var/folders/41/npjt_c8j7576_j3r6s8d71fc0000gp/T/ipykernel_53020/449568635.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_interval_data['arrival_timestamp'] = current_interval_data['arrival_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import snowflake.connector\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "DATA_FILE_PATH = '../streaming_data.parquet'\n",
    "\n",
    "# Snowflake connection configuration\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    \"account\": \"SFEDU02-KFB85562\",\n",
    "    \"user\": \"BISON\",\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"role\": \"TRAINING_ROLE\",\n",
    "    \"warehouse\": \"ANIMAL_TASK_WH\",\n",
    "    \"database\": \"CATCH_ME_IF_YOU_CAN\",\n",
    "    \"schema\": \"PUBLIC\"\n",
    "}\n",
    "\n",
    "# Simulation configuration\n",
    "SIMULATION_DURATION_MINUTES = 60  # Total simulation time in minutes\n",
    "INSERT_INTERVAL_SECONDS = 1  # Insert interval in seconds\n",
    "simulation_duration_seconds = SIMULATION_DURATION_MINUTES * 60\n",
    "\n",
    "# Get total entries without loading full file\n",
    "parquet_file = pq.ParquetFile(DATA_FILE_PATH)\n",
    "total_entries = parquet_file.metadata.num_rows\n",
    "print(f\"Total Entries: {total_entries}\")\n",
    "print(f\"Simulation Duration: {SIMULATION_DURATION_MINUTES} minutes\")\n",
    "print(f\"Interval Between Inserts: {INSERT_INTERVAL_SECONDS} seconds\")\n",
    "\n",
    "\n",
    "def process_batch(batch_df):\n",
    "    \"\"\"Clean and convert a single batch of data.\"\"\"\n",
    "    column_types = {\n",
    "        'instance_id': 'str',\n",
    "        'cluster_size': 'int',\n",
    "        'user_id': 'str',\n",
    "        'database_id': 'str',\n",
    "        'query_id': 'str',\n",
    "        'arrival_timestamp': 'datetime',\n",
    "        'compile_duration_ms': 'int',\n",
    "        'queue_duration_ms': 'int',\n",
    "        'execution_duration_ms': 'int',\n",
    "        'feature_fingerprint': 'str',\n",
    "        'was_aborted': 'bool',\n",
    "        'was_cached': 'bool',\n",
    "        'cache_source_query_id': 'str',\n",
    "        'query_type': 'str',\n",
    "        'num_permanent_tables_accessed': 'int',\n",
    "        'num_external_tables_accessed': 'int',\n",
    "        'num_system_tables_accessed': 'int',\n",
    "        'read_table_ids': 'str',\n",
    "        'write_table_ids': 'str',\n",
    "        'mbytes_scanned': 'int',\n",
    "        'mbytes_spilled': 'int',\n",
    "        'num_joins': 'int',\n",
    "        'num_scans': 'int',\n",
    "        'num_aggregations': 'int',\n",
    "        'dataset_type': 'str'\n",
    "    }\n",
    "\n",
    "    for col in batch_df.columns:\n",
    "        col_type = column_types.get(col, 'str')\n",
    "\n",
    "        # Handle null values\n",
    "        if col_type in ['int', 'float']:\n",
    "            batch_df[col] = batch_df[col].fillna(0).astype(col_type)\n",
    "        elif col_type == 'bool':\n",
    "            batch_df[col] = batch_df[col].fillna(False).astype(bool)\n",
    "        elif col_type == 'datetime':\n",
    "            batch_df[col] = pd.to_datetime(batch_df[col], errors='coerce')\n",
    "        else:  # string\n",
    "            batch_df[col] = batch_df[col].astype(str).str.strip().replace({'nan': '', 'None': '', 'null': ''}).fillna('')\n",
    "    return batch_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing loop for simulating data insertion.\"\"\"\n",
    "    parquet_file = pq.ParquetFile(DATA_FILE_PATH)\n",
    "\n",
    "    # Get the earliest and latest arrival timestamps\n",
    "    try:\n",
    "        first_batch_arrow = next(parquet_file.iter_batches(batch_size=1))\n",
    "        first_batch_df = process_batch(first_batch_arrow.to_pandas())\n",
    "        original_start_time = pd.to_datetime(first_batch_df['arrival_timestamp'].iloc[0])\n",
    "\n",
    "        num_row_groups = parquet_file.num_row_groups\n",
    "        last_row_group_table = parquet_file.read_row_group(num_row_groups - 1)\n",
    "        last_row_group_df = process_batch(last_row_group_table.to_pandas())\n",
    "        original_end_time = pd.to_datetime(last_row_group_df['arrival_timestamp'].iloc[-1])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading timestamps from parquet file: {e}\")\n",
    "        return\n",
    "\n",
    "    original_time_span_seconds = (original_end_time - original_start_time).total_seconds()\n",
    "    if original_time_span_seconds == 0:\n",
    "        print(\"Error: Original data time span is zero.\")\n",
    "        return\n",
    "\n",
    "    compression_factor = original_time_span_seconds / simulation_duration_seconds\n",
    "    print(f\"Original Data Time Span: {original_time_span_seconds} seconds\")\n",
    "    print(f\"Compression Factor: {compression_factor}\")\n",
    "\n",
    "    start_date_str = original_start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"Simulation Data Starts and Static Data Ends at: {start_date_str}\")\n",
    "\n",
    "    with snowflake.connector.connect(**SNOWFLAKE_CONFIG) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            while True:\n",
    "                # Delete previous data in live_queries with arrival_timestamp >= start_date\n",
    "                delete_sql = f\"DELETE FROM live_queries WHERE arrival_timestamp >= '{start_date_str}'\"\n",
    "                try:\n",
    "                    cur.execute(delete_sql)\n",
    "                    delete_count = cur.rowcount\n",
    "                    conn.commit()\n",
    "                    print(f\"Deleted {delete_count} rows from previous simulation.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting previous data: {e}\")\n",
    "                    continue  # Skip to the next iteration\n",
    "\n",
    "                pending_data = pd.DataFrame()\n",
    "                batch_iter = parquet_file.iter_batches(batch_size=10000)\n",
    "                simulation_time = 0\n",
    "                data_iterator_exhausted = False\n",
    "\n",
    "                total_simulation_steps = int(simulation_duration_seconds / INSERT_INTERVAL_SECONDS) + 1\n",
    "                progress_bar = tqdm(total=total_simulation_steps, desc=\"Simulation Time\")\n",
    "\n",
    "                while simulation_time <= simulation_duration_seconds:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Compute original time window for current interval\n",
    "                    original_start_time_interval = original_start_time + pd.Timedelta(seconds=simulation_time * compression_factor)\n",
    "                    original_end_time_interval = original_start_time + pd.Timedelta(seconds=(simulation_time + INSERT_INTERVAL_SECONDS) * compression_factor)\n",
    "\n",
    "                    # Read data batches until we have all data up to original_end_time_interval\n",
    "                    while not data_iterator_exhausted:\n",
    "                        if pending_data.empty or pending_data['arrival_timestamp'].max() < original_end_time_interval:\n",
    "                            try:\n",
    "                                arrow_batch = next(batch_iter)\n",
    "                                raw_batch_df = arrow_batch.to_pandas()\n",
    "                                batch_df = process_batch(raw_batch_df)\n",
    "                                pending_data = pd.concat([pending_data, batch_df], ignore_index=True)\n",
    "                            except StopIteration:\n",
    "                                data_iterator_exhausted = True\n",
    "                                break\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    # Select data for current interval\n",
    "                    mask = (pending_data['arrival_timestamp'] >= original_start_time_interval) & (pending_data['arrival_timestamp'] < original_end_time_interval)\n",
    "                    current_interval_data = pending_data.loc[mask]\n",
    "\n",
    "                    if not current_interval_data.empty:\n",
    "                        # Prepare data for insertion\n",
    "                        current_interval_data['arrival_timestamp'] = current_interval_data['arrival_timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        \n",
    "                        # Convert to list of tuples for SQL insertion\n",
    "                        rows = [tuple(row) for row in current_interval_data.to_numpy()]\n",
    "\n",
    "                        # Generate bulk INSERT query\n",
    "                        columns = ', '.join(current_interval_data.columns)\n",
    "                        placeholders_per_row = ', '.join(['%s'] * len(current_interval_data.columns))\n",
    "                        values_placeholders = ', '.join([f'({placeholders_per_row})' for _ in rows])\n",
    "\n",
    "                        sql = f\"INSERT INTO live_queries ({columns}) VALUES {values_placeholders}\"\n",
    "\n",
    "                        # Flatten rows into a single parameter list\n",
    "                        params = [param for row in rows for param in row]\n",
    "\n",
    "                        # Execute in one go\n",
    "                        try:\n",
    "                            cur.execute(sql, params)\n",
    "                            conn.commit()\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error inserting data at simulation time {simulation_time}: {e}\")\n",
    "\n",
    "\n",
    "                    # Remove inserted data from pending_data\n",
    "                    pending_data = pending_data.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "                    # Sleep for any remaining time in interval\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    sleep_time = max(0, INSERT_INTERVAL_SECONDS - elapsed_time)\n",
    "                    time.sleep(sleep_time)\n",
    "\n",
    "                    simulation_time += INSERT_INTERVAL_SECONDS\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "                    # If data iterator is exhausted and pending_data is empty, we can break early\n",
    "                    if data_iterator_exhausted and pending_data.empty:\n",
    "                        break\n",
    "\n",
    "                progress_bar.close()\n",
    "                print(\"Simulation complete. Restarting...\")\n",
    "                time.sleep(5)  # Sleep for 5 seconds before restarting\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Replace Table (DONT RUN IT WILL DELETE ALL THE DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "# to use create a .env file in the root directory and insert the following:\n",
    "# SNOWFLAKE_PASSWORD=your_password\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "\n",
    "# Snowflake connection configuration\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    \"account\": \"SFEDU02-KFB85562\",\n",
    "    \"user\": \"BISON\",\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"role\": \"TRAINING_ROLE\",\n",
    "    \"warehouse\": \"ANIMAL_TASK_WH\",\n",
    "    \"database\": \"CATCH_ME_IF_YOU_CAN\",\n",
    "    \"schema\": \"PUBLIC\"\n",
    "}\n",
    "\n",
    "input = input(\"Do you really want to create the table? (yes/no): \")\n",
    "if input != \"yes\":\n",
    "    print(\"Aborting table creation.\")\n",
    "    exit()\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
    "cur = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE live_queries (\n",
    "    instance_id VARCHAR,\n",
    "    cluster_size INTEGER,\n",
    "    user_id VARCHAR,\n",
    "    database_id VARCHAR,\n",
    "    query_id VARCHAR,\n",
    "    arrival_timestamp TIMESTAMP,\n",
    "    compile_duration_ms INTEGER,\n",
    "    queue_duration_ms INTEGER,\n",
    "    execution_duration_ms INTEGER,\n",
    "    feature_fingerprint VARCHAR,\n",
    "    was_aborted BOOLEAN,\n",
    "    was_cached BOOLEAN,\n",
    "    cache_source_query_id VARCHAR,\n",
    "    query_type VARCHAR,\n",
    "    num_permanent_tables_accessed INTEGER,\n",
    "    num_external_tables_accessed INTEGER,\n",
    "    num_system_tables_accessed INTEGER,\n",
    "    read_table_ids VARCHAR,\n",
    "    write_table_ids VARCHAR,\n",
    "    mbytes_scanned INTEGER,\n",
    "    mbytes_spilled INTEGER,\n",
    "    num_joins INTEGER,\n",
    "    num_scans INTEGER,\n",
    "    num_aggregations INTEGER,\n",
    "    dataset_type VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Snowflake table created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
